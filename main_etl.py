import sys
import os
import logging
import time

# AÃ±adir la ruta del proyecto para importar mÃ³dulos (necesario en algunos entornos)
# sys.path.append(os.getcwd())

PROJECT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_DIR)

# Importar mÃ³dulos ETL
from core.spark_session import create_spark_session
from etl_modules.extract import extract_data
from etl_modules.transform import transform_data
from etl_modules.load import load_data, get_max_timestamp_from_target

# Forzar logs a usar tu hora local
os.environ["TZ"] = "America/La_Paz"
time.tzset()

# Configurar logging (opcional, pero buena prÃ¡ctica)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def run_full_etl():
    """Ejecuta el flujo completo de ETL: Extract -> Transform -> Load."""
    spark = None
    try:
        logger.info("ğŸš€ Iniciando proceso ETL (Incremental)...")
        
        # 1. Crear sesiÃ³n de Spark
        logger.info("ğŸ”¥ Inicializando sesiÃ³n de Spark...")
        spark = create_spark_session()
        logger.info("ğŸ”¥ SesiÃ³n de Spark inicializada.")
        
        # 2. PREPARACIÃ“N: Obtener la marca de tiempo de la Ãºltima ejecuciÃ³n
        logger.info("â±ï¸ Obteniendo Ãºltima marca de tiempo...")
        last_ts = get_max_timestamp_from_target(spark)
        logger.info(f"ğŸ“Œ Ãšltima marca de tiempo cargada en destino: {last_ts}")
        
        # 3. Extract: Leer de la DB Origen con filtro incremental
        logger.info("ğŸ“¥ Extrayendo datos desde Supabase...")
        df_raw = extract_data(spark, last_ts, origin="supabase")
        records_extracted = df_raw.count()
        logger.info(f"ğŸ“¥ ExtracciÃ³n completa: {records_extracted} registros nuevos.")
        
        # ValidaciÃ³n: Si no hay filas nuevas, detener el proceso
        if records_extracted == 0:
            logger.info("âš ï¸ No existen registros nuevos. Proceso ETL finalizado.")
            return
        
        # 3. Transform: Aplicar lÃ³gica de negocio
        logger.info("ğŸ”„ Iniciando transformaciÃ³n de datos...")
        df_transformed = transform_data(df_raw, spark)
        logger.info(f"ğŸ”„ TransformaciÃ³n completada: {df_transformed.count()} registros listos para cargar.")
        
        # 4. Load: Escribir en PostgreSQL
        logger.info("ğŸ“¤ Cargando datos en PostgreSQL...")
        load_data(df_transformed)
        logger.info("âœ… Ã‰XITO: Datos cargados correctamente en PostgreSQL.")

    except Exception as e:
        logger.error(f"âŒ Error crÃ­tico en el proceso ETL: {e}", exc_info=True)
        sys.exit(1) 

    finally:
        if spark:
            spark.stop()
            logger.info("ğŸ SesiÃ³n de Spark finalizada. ETL completado.")

if __name__ == "__main__":
    run_full_etl()